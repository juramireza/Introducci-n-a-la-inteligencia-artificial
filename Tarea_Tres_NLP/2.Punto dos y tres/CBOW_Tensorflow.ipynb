{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CBOW_Tensorflow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPree6T8aL5wxQ9g6UIlylw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#1.Importar librerías"],"metadata":{"id":"xQwbis6yOVUX"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"k5e9waN2OP8E","executionInfo":{"status":"ok","timestamp":1655314854617,"user_tz":300,"elapsed":209,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"outputs":[],"source":["from nltk.corpus import gutenberg # obtener el corpus de la biblia\n","from string import punctuation # remover la puntiación \n","import nltk \n","import re\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing import text\n","from keras.preprocessing.sequence import skipgrams \n","from keras.layers import *\n","from keras.layers.core import Dense, Reshape\n","from keras.layers.embeddings import Embedding\n","from keras.models import Model,Sequential\n","\n","pd.options.display.max_colwidth = 200\n","%matplotlib inline"]},{"cell_type":"markdown","source":["#2.Descarga del proyecto gutenberg, el modelo puntkt y las palabras vacías "],"metadata":{"id":"0E4DosqeRriE"}},{"cell_type":"code","source":["nltk.download('gutenberg')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = nltk.corpus.stopwords.words('english')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-rDBeIpPI0c","executionInfo":{"status":"ok","timestamp":1655314855820,"user_tz":300,"elapsed":219,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"2a66ae09-b197-4db4-a9f0-6a25b0bd95db"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["#3.Limpieza del corpus documental y tokenización"],"metadata":{"id":"CagQLJP9RvoS"}},{"cell_type":"code","source":["bible = gutenberg.sents(\"bible-kjv.txt\")\n","remove_terms = punctuation + '0123456789'"],"metadata":{"id":"NWBTl1RVQN8s","executionInfo":{"status":"ok","timestamp":1655314857963,"user_tz":300,"elapsed":6,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["wpt = nltk.WordPunctTokenizer()\n","\n","def normalize_document(doc):\n","    # Se remueven carácteres especiales y espacios \n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # Se tokeniza el documento\n","    tokens = wpt.tokenize(doc)\n","    # Se filtran las palabras vacías \n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # se reconstruye el docuemento tokenizado\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","normalize_corpus = np.vectorize(normalize_document)"],"metadata":{"id":"rWd_aC_YPIO9","executionInfo":{"status":"ok","timestamp":1655314859115,"user_tz":300,"elapsed":271,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n","norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n","norm_bible = filter(None, normalize_corpus(norm_bible))\n","norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n","\n","print('Total lines:', len(bible))\n","print('\\nSample line:', bible[10])\n","print('\\nProcessed line:', norm_bible[10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wg_bVBCXP7Zt","executionInfo":{"status":"ok","timestamp":1655314869875,"user_tz":300,"elapsed":9426,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"68e865c8-0fc4-4a08-b2fa-352ee781ff5c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total lines: 30103\n","\n","Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n","\n","Processed line: god said let firmament midst waters let divide waters waters\n"]}]},{"cell_type":"markdown","source":["#4.Construcción del vocabulario "],"metadata":{"id":"MIhy7vhuR7hs"}},{"cell_type":"code","source":["from keras.preprocessing import text\n","from keras.utils import np_utils\n","from keras.preprocessing import sequence\n","\n","tokenizer = text.Tokenizer()\n","tokenizer.fit_on_texts(norm_bible)\n","word2id = tokenizer.word_index\n","\n","word2id['PAD'] = 0\n","id2word = {v:k for k, v in word2id.items()}\n","wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n","\n","vocab_size = len(word2id)\n","embed_size = 100\n","window_size = 2\n","\n","print('Vocabulary Size:', vocab_size)\n","print('Vocabulary Sample:', list(word2id.items())[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ws6hmmZgQbN8","executionInfo":{"status":"ok","timestamp":1655314871279,"user_tz":300,"elapsed":1416,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"8311b40b-aced-45e1-912e-66940c135b27"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 12425\n","Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"]}]},{"cell_type":"markdown","source":["#5.Construcción de context_words y target_word"],"metadata":{"id":"Ql2PhTLsSeFL"}},{"cell_type":"code","source":["def generate_context_word_pairs(corpus, window_size, vocab_size):\n","    context_length = window_size*2\n","    for words in corpus:\n","        sentence_length = len(words)\n","        for index, word in enumerate(words):\n","            context_words = []\n","            label_word   = []            \n","            start = index - window_size\n","            end = index + window_size + 1\n","            \n","            context_words.append([words[i] \n","                                 for i in range(start, end) \n","                                 if 0 <= i < sentence_length \n","                                 and i != index])\n","            label_word.append(word)\n","\n","            x = sequence.pad_sequences(context_words, maxlen=context_length)\n","            y = np_utils.to_categorical(label_word, vocab_size)\n","            yield (x, y)"],"metadata":{"id":"Zx0wKDziQd1T","executionInfo":{"status":"ok","timestamp":1655314871281,"user_tz":300,"elapsed":32,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["i = 0\n","for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n","    if 0 not in x[0]:\n","        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n","    \n","        if i == 10:\n","            break\n","        i += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmJEmdF8OcPd","executionInfo":{"status":"ok","timestamp":1655314871281,"user_tz":300,"elapsed":29,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"382fffa9-909d-42e4-85b4-bdc95c2ac148"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n","Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n","Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n","Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n","Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n","Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n","Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n","Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n","Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n","Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n","Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"]}]},{"cell_type":"markdown","source":["#6.Construcción del modelo de redes neuronales de CBOW"],"metadata":{"id":"mtUHDLhRSrgG"}},{"cell_type":"code","source":["import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","\n","cbow = Sequential()\n","cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n","cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n","cbow.add(Dense(vocab_size, activation='softmax'))\n","\n","cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","print(cbow.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh_H-B-lQrVz","executionInfo":{"status":"ok","timestamp":1655314871282,"user_tz":300,"elapsed":25,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"0c8a7fdc-097c-4a20-c8c3-2379f44770e1"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 4, 100)            1242500   \n","                                                                 \n"," lambda_1 (Lambda)           (None, 100)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 12425)             1254925   \n","                                                                 \n","=================================================================\n","Total params: 2,497,425\n","Trainable params: 2,497,425\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n","                 rankdir='TB').create(prog='dot', format='svg'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"id":"5SOURnTDQ5M0","executionInfo":{"status":"ok","timestamp":1655314871499,"user_tz":300,"elapsed":20,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}},"outputId":"56bfec85-8ecf-491c-fc47-a85b56629bbb"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg height=\"405pt\" viewBox=\"0.00 0.00 324.00 304.00\" width=\"432pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 320,-300 320,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140288675672784 -->\n<g class=\"node\" id=\"node1\">\n<title>140288675672784</title>\n<polygon fill=\"none\" points=\"8,-249.5 8,-295.5 308,-295.5 308,-249.5 8,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"48\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"88,-249.5 88,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"88,-272.5 146,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"146,-249.5 146,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-268.8\">[(None, 4)]</text>\n<polyline fill=\"none\" points=\"227,-249.5 227,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-268.8\">[(None, 4)]</text>\n</g>\n<!-- 140288676657360 -->\n<g class=\"node\" id=\"node2\">\n<title>140288676657360</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 316,-212.5 316,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-185.8\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-166.5 84,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-189.5 142,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-166.5 142,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178\" y=\"-185.8\">(None, 4)</text>\n<polyline fill=\"none\" points=\"214,-166.5 214,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-185.8\">(None, 4, 100)</text>\n</g>\n<!-- 140288675672784&#45;&gt;140288676657360 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140288675672784-&gt;140288676657360</title>\n<path d=\"M158,-249.3799C158,-241.1745 158,-231.7679 158,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"161.5001,-222.784 158,-212.784 154.5001,-222.784 161.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140288669182032 -->\n<g class=\"node\" id=\"node3\">\n<title>140288669182032</title>\n<polygon fill=\"none\" points=\"2.5,-83.5 2.5,-129.5 313.5,-129.5 313.5,-83.5 2.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"34.5\" y=\"-102.8\">Lambda</text>\n<polyline fill=\"none\" points=\"66.5,-83.5 66.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"66.5,-106.5 124.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"124.5,-83.5 124.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-102.8\">(None, 4, 100)</text>\n<polyline fill=\"none\" points=\"226.5,-83.5 226.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"270\" y=\"-102.8\">(None, 100)</text>\n</g>\n<!-- 140288676657360&#45;&gt;140288669182032 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140288676657360-&gt;140288669182032</title>\n<path d=\"M158,-166.3799C158,-158.1745 158,-148.7679 158,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"161.5001,-139.784 158,-129.784 154.5001,-139.784 161.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140288675672144 -->\n<g class=\"node\" id=\"node4\">\n<title>140288675672144</title>\n<polygon fill=\"none\" points=\"8.5,-.5 8.5,-46.5 307.5,-46.5 307.5,-.5 8.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"34.5\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"60.5,-.5 60.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"60.5,-23.5 118.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"118.5,-.5 118.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-19.8\">(None, 100)</text>\n<polyline fill=\"none\" points=\"205.5,-.5 205.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-19.8\">(None, 12425)</text>\n</g>\n<!-- 140288669182032&#45;&gt;140288675672144 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140288669182032-&gt;140288675672144</title>\n<path d=\"M158,-83.3799C158,-75.1745 158,-65.7679 158,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"161.5001,-56.784 158,-46.784 154.5001,-56.784 161.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["#7.Entrenamiento del modelo "],"metadata":{"id":"LoKhHpAWTZSy"}},{"cell_type":"code","source":["for epoch in range(1, 2):\n","    loss = 0.\n","    i = 0\n","    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n","        i += 1\n","        loss += cbow.train_on_batch(x, y)\n","        if i % 100000 == 0:\n","            print('Processed {} (context, word) pairs'.format(i))\n","\n","    print('Epoch:', epoch, '\\tLoss:', loss)\n","    print()"],"metadata":{"id":"o0pg8x41TMqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#8.Obtener los embeddings de las palabras"],"metadata":{"id":"BqMin0EaTedV"}},{"cell_type":"code","source":["weights = cbow.get_weights()[0]\n","weights = weights[1:]\n","print(weights.shape)\n","\n","pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"],"metadata":{"id":"Chws8RSwRCVq","executionInfo":{"status":"aborted","timestamp":1655314847446,"user_tz":300,"elapsed":3,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#9.Matriz de distancia entre palabras similares"],"metadata":{"id":"Wd6nuoObTmUg"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import euclidean_distances\n","\n","# compute pairwise distance matrix\n","distance_matrix = euclidean_distances(weights)\n","print(distance_matrix.shape)\n","\n","# view contextually similar words\n","similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n","                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n","\n","similar_words"],"metadata":{"id":"6OTgKqqXRFN3","executionInfo":{"status":"aborted","timestamp":1655314847447,"user_tz":300,"elapsed":4,"user":{"displayName":"Juan David Ramirez Avila","userId":"17968284916782554478"}}},"execution_count":null,"outputs":[]}]}